def train_epoch(self):
    """Train for one epoch with separate gradient flows for each head"""
    self.model.train()
    total_losses = {'total_loss': 0, 'action_loss': 0, 'query_loss': 0, 'coord_loss': 0}
    num_batches = 0
    
    with tqdm(self.train_loader, desc="Training", 
             bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}') as pbar:
        for batch in pbar:
            # Move batch to device
            batch = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()}
            
            # Zero gradients
            self.optimizer.zero_grad()
            
            # Forward pass
            outputs = self.model.forward_training(
                {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}
            )
            
            # Prepare targets
            targets = {
                'action_labels': batch['action_label'],
                'query_labels': batch['query_labels'],
                'touch_coords': batch['touch_coords'],
                'lift_coords': batch['lift_coords'],
                'bounding_boxes': batch['bounding_boxes']
            }
            
            # Compute losses
            losses = self.loss_fn(outputs, targets)
            
            # Create dummy zero tensor for gradient accumulation
            dummy_zero = torch.tensor(0.0, requires_grad=True, device=self.device)
            
            # Action loss backward (only affects action head)
            if torch.isfinite(losses['action_loss']):
                (dummy_zero + self.config['action_weight'] * losses['action_loss']).backward(retain_graph=True)
            
            # Query loss backward (only affects query head)
            if torch.isfinite(losses['query_loss']):
                (dummy_zero + self.config['query_weight'] * losses['query_loss']).backward(retain_graph=True)
            
            # Coordinate loss backward (only affects coordinate head)
            if torch.isfinite(losses['coord_loss']):
                (dummy_zero + self.config['coord_weight'] * losses['coord_loss']).backward()
            
            # Clip gradients and update
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # Track losses
            for key in total_losses:
                if key in losses:
                    total_losses[key] += losses[key].item()
            num_batches += 1
            
            # Update progress bar
            pbar.set_postfix({
                'Loss': f"{losses['total_loss'].item():.4f}",
                'Action': f"{losses.get('action_loss', 0):.4f}",
                'Query': f"{losses.get('query_loss', 0):.4f}",
                'Coord': f"{losses.get('coord_loss', 0):.4f}"
            })
    
    return {k: v / num_batches for k, v in total_losses.items()}




###

I've identified several key issues in your implementation that could be causing poor performance. Here are the critical problems and solutions:

### 1. **Model Architecture Issues**
**Problem**: The coordinate prediction LSTM is unnecessarily complex and autoregressive for simple coordinate prediction. Gesture coordinates (x1,y1,x2,y2) don't need sequential generation.

**Solution**: Replace LSTM with a simple MLP:
```python
# Replace gesture LSTM with MLP
self.gesture_coord = nn.Sequential(
    nn.Linear(bert_output_size + self.lstm_hidden_size, lstm_hidden_size),
    nn.ReLU(),
    nn.Linear(lstm_hidden_size, 4),  # Directly predict 4 coordinates
    nn.Sigmoid()  # Normalize to [0,1]
)
```

### 2. **Loss Function Problems**
**Problem**: The custom coordinate loss has non-differentiable operations (`is_tap`, `point_in_box`) and complex penalties that prevent gradient flow.

**Solution**: Simplify to standard losses:
```python
class ADCLoss(nn.Module):
    def __init__(self, action_weight=1.0, query_weight=0.5, coord_weight=2.0):
        super().__init__()
        self.action_loss_fn = nn.CrossEntropyLoss()
        self.query_loss_fn = nn.CrossEntropyLoss(ignore_index=0)
        self.coord_loss_fn = nn.HuberLoss()  # Robust coordinate loss
        self.action_weight = action_weight
        self.query_weight = query_weight
        self.coord_weight = coord_weight

    def forward(self, outputs, targets):
        # Action loss
        action_loss = self.action_loss_fn(
            outputs['action_logits'], 
            targets['action_labels']
        )
        
        # Query loss
        query_logits = outputs['query_logits'].view(-1, outputs['query_logits'].size(-1))
        query_targets = targets['query_labels'].view(-1)
        query_loss = self.query_loss_fn(query_logits, query_targets)
        
        # Coordinate loss (only for gesture actions)
        gesture_mask = torch.isin(targets['action_labels'], torch.tensor([4,5,6,7,10,11]))
        if gesture_mask.any():
            coord_loss = self.coord_loss_fn(
                outputs['coordinates'][gesture_mask],
                torch.cat([
                    targets['touch_coords'][gesture_mask],
                    targets['lift_coords'][gesture_mask]
                ], dim=1)
            )
        else:
            coord_loss = torch.tensor(0.0)
        
        # Weighted total
        total_loss = (
            self.action_weight * action_loss +
            self.query_weight * query_loss +
            self.coord_weight * coord_loss
        )
        
        return {
            'total_loss': total_loss,
            'action_loss': action_loss,
            'query_loss': query_loss,
            'coord_loss': coord_loss
        }
```

### 3. **Training Loop Flaws**
**Problem**: Multiple backward passes cause gradient conflicts and instability.

**Solution**: Use a single backward pass:
```python
# Inside train_epoch()
total_loss = losses['total_loss']
total_loss.backward()  # SINGLE backward pass
torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
self.optimizer.step()
```

### 4. **Action-Specific Output Handling**
**Problem**: The model generates outputs for all actions regardless of type, creating noise.

**Solution**: Modify output generation:
```python
# In forward_training()
if action_code.item() == 3:  # Search action
    outputs['query_logits'] = self._generate_search_query_training(...)
elif action_code.item() in {4,5,6,7,10,11}:  # Gesture actions
    outputs['coordinates'] = self._generate_gesture_coords(...)
```

### 5. **Data Normalization**
**Problem**: Coordinates aren't normalized, causing sigmoid outputs to be ineffective.

**Solution**: Add normalization in dataset:
```python
# In ADCDataset.__getitem__
touch_coords = torch.tensor(entry.results.touch_coords, dtype=torch.float32) / SCREEN_SIZE
lift_coords = torch.tensor(entry.results.lift_coords, dtype=torch.float32) / SCREEN_SIZE
```

### 6. **Critical Bug: Multi-Head Interference**
**Problem**: All heads share the same BERT output without separation.

**Solution**: Use separate projections:
```python
# Add distinct projections
self.action_proj = nn.Linear(bert_output_size, bert_output_size)
self.query_proj = nn.Linear(bert_output_size, bert_output_size)
self.coord_proj = nn.Linear(bert_output_size, bert_output_size)

# In forward_training()
action_input = self.action_proj(pooled_output)
query_input = self.query_proj(pooled_output)
coord_input = self.coord_proj(pooled_output)
```

### 7. **Additional Improvements**
1. **Warmup Scheduler**:
   ```python
   from torch.optim.lr_scheduler import LambdaLR
   
   # Add to trainer initialization
   self.warmup_scheduler = LambdaLR(
       self.optimizer,
       lr_lambda=lambda epoch: min(1.0, epoch / 10)  # 10-epoch warmup
   )
   ```

2. **Gradient Clipping**:
   ```python
   torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
   ```

3. **LayerNorm** after BERT:
   ```python
   self.post_bert_norm = nn.LayerNorm(bert_output_size)
   ```

### Implementation Plan:
1. Simplify coordinate prediction to MLP
2. Fix loss function with standard losses
3. Implement single backward pass
4. Add action-specific output masking
5. Normalize coordinate targets
6. Separate projection heads
7. Add warmup scheduler
8. Add gradient clipping

These changes should significantly improve performance by:
- Removing non-differentiable operations
- Preventing gradient conflicts
- Isolating task-specific features
- Stabilizing training dynamics
- Improving coordinate prediction accuracy

Would you like me to provide the full corrected code for any specific module?
